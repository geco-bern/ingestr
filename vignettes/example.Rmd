---
title: "Examples for ingestr"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r setup, include=FALSE}
library(ingestr)
library(tidyverse)
```

# Overview

The package `ingestr` provides functions to extract (ingest) point data (given longitude, latitude, and required dates) from large global files or remote data servers and create time series at user-specified temporal resolution. This can be done for a set of sites at once, given a data frame containing the meta info for each site (see data frame `siteinfo`, with columns `lon` for longitude, `lat` for latitude, `date_start` and `date_end` specifying required dates). The output for such a set of site-level data is a nested data frame with rows for each site and columns `lon`, `lat`, `date_start`, and `date_end` plus an added column where the time series of ingested data is nested inside.

Data can be ingested for different data types (argument `source` in functions `ingest()` and `ingest_bysite()`, see Column Source ID in table below). For each data type, functions deal with a specific format of the original data and specific functions to read from respective files or remote servers. The following data types can be handled currently (more to be added by you if you like):

| Data source                                                          | Data type                                | Coverage | Source ID     | Reading from  | Remark     |
|-------------------------                                             |---------------                           |--------- |---------------| ---           |---         |
| [FLUXNET](https://fluxnet.fluxdata.org/data/fluxnet2015-dataset/)    | ecosystem fluxes, meteo, soil moisture   | site     | `fluxnet`     | local files   |            |
| [WATCH-WFDEI](http://www.eu-watch.org/data_availability)             | meteo                                    | global   | `watch_wfdei` | local files   |            |
| [CRU](https://crudata.uea.ac.uk/cru/data/hrg/)                       | meteo                                    | global   | `cru`         | local files   |            |
| MODIS LP DAAC                                                        | remote sensing                           | global   | `modis`       | remote server | using [*MODISTools*](https://docs.ropensci.org/MODISTools/) |
| Google Earth Engine                                                  | remote sensing                           | global   | `gee`         | remote server | using Koen Hufken's [gee_suset](https://khufkens.github.io/gee_subset/) library |
| [ETOPO1](https://www.ngdc.noaa.gov/mgg/global/)                      | elevation                                | global   | `etopo1`      | local files   |            |
| [Mauna Loa CO2](https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html) | CO2 concentration                        | site     | `co2_mlo`     | remote server | using the [climate](https://github.com/bczernecki/climate) R package |
| HWSD                                                                 | soil                                     | global   | `hwsd`        | local files   | using an adaption of David Le Bauer's [rhwsd](https://github.com/dlebauer/rhwsd) R package |
| [WWF Ecoregions](https://databasin.org/datasets/68635d7c77f1475f9b6c1d1dbe0a4c4c) | vegetation classification   | global   | `wwf`         | local files   | Olsen et al. (2001)| 

Examples to read data for a single site for each data type are given in Section 'Examples for a single site'. Handling ingestion for multiple sites is descrbed in Section 'Example for a set of sites'.
**Note** that this package does not provide the original data. Please follow links to data sources above where data is read from local files, and always cite original references.

## Variable names and units

All ingested data follows standardised variable naming and (optionally) units. 

| Variable                           | Variable name | Units                          |
|-------------------------           |---------------|---------------                 |
| Gross primary production           | `gpp`         | g CO$^{-2}$ m$^{-2}$ s$^{-1}$  |
| Air temperature                    | `temp`        | $^\circ$C                      |
| Precipitation                      | `prec`        | mm s$^{-1}$                    |
| Vapour pressure deficit            | `vpd`         | Pa                             |
| Atmospheric pressure               | `patm`        | Pa                             |
| Net radiation                      | `netrad`      | J m$^{-2}$ s$^{-1}$            |
| Photosynthetic photon flux density | `ppfd`        | mol m$^{-2}$ s$^{-1}$          |
| Elevation (altitude)               | `elv`         | m a.s.l.                       |        

Use these variable names for specifying which variable names they correspond to in the original data source (see argument `getvars` to functions `ingest()` and `ingest_bysite()`).

# Examples for a single site

The function `ingest_bysite()` can be used to ingest data for a single site. The argument `source` specifies which data type (source) is to be read from and triggers the use of specific wrapper functions that are designed to read from files whith formats that differ between sources. Source-specific settings for data processing can be provided by argument `settings` (described for each data source below). More info about other, source-independent arguments are available through the man page (see `?ingest_bysite`).

## FLUXNET

### Meteo data

Reading from FLUXNET files offers multiple settings to be used specified by the user. Here, we're specifying that no soil water content data is read (`getswc = FALSE` in `settings_fluxnet`, passed to `ingest_bysite()` through argument `settings`).
```{r message=FALSE}
settings_fluxnet <- list(getswc = FALSE)

df_fluxnet <- ingest_bysite(
  sitename = "FR-Pue",
  source = "fluxnet",
  getvars = list(temp = "TA_F",
                 prec = "P_F",
                 vpd  = "VPD_F",
                 ppfd =  "SW_IN_F",
                 netrad = "NETRAD",
                 patm = "PA_F"),
  dir = paste0(path.package("ingestr"), "/extdata/"),
  settings = settings_fluxnet,
  timescale = "d",
  year_start = 2007,
  year_end = 2007,
  verbose = FALSE
  )
df_fluxnet
```

Note that the argument `getvars` as specified above triggers the ingestion of the six variables `"TA_F", "P_F", "VPD_F",  "SW_IN_F", "NETRAD", "PA_F"` and their renaming to `"temp", "prec", "vpd", "ppfd", "netrad", "patm"`, respecitvely.

### Flux data

The same function can also be used to read in other FLUXNET variables (e.g., CO2 flux data) and conduct data filtering steps. Here, we're reading daily GPP and uncertainty (standard error), based on the nighttime flux decomposition method (`"GPP_NT_VUT_REF"` and `"GPP_NT_VUT_SE"` in argument `getvars`). The `settings` argument can be used again to specify settings that are specific to the `"fluxnet"` data source. Here, we keep only data where at least 80% is based on non-gapfilled half-hourly data (`threshold_GPP = 0.8`), and where the daytime and nighttime-based estimates are consistent, that is, where their difference is below the the 97.5% and above the 2.5% quantile (`filter_ntdt = TRUE`). Negative GPP values are not removed (`remove_neg = FALSE`). We read data for just one year here (2007).
```{r warning=FALSE, message=FALSE}
settings_fluxnet <- list(
  getswc       = FALSE,
  filter_ntdt  = TRUE,
  threshold_GPP= 0.8,
  remove_neg   = FALSE
  )

ddf_fluxnet <- ingest_bysite(
  sitename  = "FR-Pue",
  source    = "fluxnet",
  getvars   = list( gpp = "GPP_NT_VUT_REF",
                    gpp_unc = "GPP_NT_VUT_SE"),
  dir       = paste0(path.package("ingestr"), "/extdata/"),
  settings  = settings_fluxnet,
  timescale = "d",
  year_start = 2007,
  year_end  = 2007
  )
```


### Settings

The argument `settings` in functions `ingest_bysite()` and `ingest()` is used to pass settings that are specific to the data source (argument `source`) with which the functions are used. Default settings are specified for each data source. For `source = "fluxnet"`, defaults are returned by a function call of `get_settings_fluxnet()` and are described in the function's man page (see `?get_settings_fluxnet`). Defaults are used for settings elements that are not specified by the user.

## WATCH-WFDEI

Let's extract data for the location corresponding to FLUXNET site 'CH-Lae' (lon = 8.365, lat = 47.4781). This extracts from original WATCH-WFDEI files, provided as NetCDF (global, 0.5 degree resolution), provided as monthly files containing all days in each month. The data directory specified here (`dir = "~/data/watch_wfdei/"`) contains subdirectories with names containing the variable names (corresponding to the ones specified by the argument `getvars = list(temp = "Tair")`). We read data for just one year here (2007).
```{r message=FALSE, echo = T, results = 'hide'}
df_watch <- ingest_bysite(
  sitename  = "FR-Pue",
  source    = "watch_wfdei",
  getvars   = list(temp = "Tair"),
  dir       = "~/data/watch_wfdei/",
  timescale = "d",
  year_start = 2007,
  year_end  = 2007,
  lon       = 3.5958,
  lat       = 43.7414,
  verbose   = FALSE
  )
df_watch
```

## CRU TS

As above, let's extract CRU data for the location corresponding to FLUXNET site 'FR-Pue' (lon = 8.365, lat = 47.4781). Note that we're using `tmx` (the daily maximum temperature). This extracts monthly data from the CRU TS data. Interpolation to daily values is done using a wather generator for daily precipitation (given monthly total precipitation and number of wet days in each month), and a polynomial that conserves monthly means for all other variables.
```{r message=FALSE}
df_cru <- ingest_bysite(
  sitename  = "FR-Pue",
  source    = "cru",
  getvars   = list(temp = "tmx"),
  dir       = "~/data/cru/ts_4.01/",
  timescale = "d",
  year_start = 2007,
  year_end  = 2007,
  lon       = 3.5958,
  lat       = 43.7414,
  verbose   = FALSE
  )
df_cru
```

We can compare the temperature recorded at the site and the temperature data extracted from WATCH-WFDEI.
```{r}
df <- df_fluxnet %>%
  rename(temp_fluxnet = temp) %>%
  left_join(rename(df_watch, temp_watch = temp), by = c("sitename", "date")) %>%
  left_join(rename(df_cru, temp_cru = temp), by = c("sitename", "date")) %>%
  pivot_longer(cols = c(temp_fluxnet, temp_watch, temp_cru), names_to = "source", values_to = "temp", names_prefix = "temp_")

library(ggplot2)
df %>%
  ggplot(aes(x = date, y = temp, color = source)) +
  geom_line()
  xlim(c("2007-01-01", "2007-12-31"))
```

Looks sweet.

## MODIS LP DAAC

This uses the [*MODISTools*](https://docs.ropensci.org/MODISTools/) R package making its interface consistent with ingestr. Settings can be specified and passed on using the `settings` argument. To facilitate the selection of data products and bands to be downloaded, you may use the function `get_settings_modis)` which defines defaults for different data bundles (`c("modis_fpar", "modis_ndvi", "modis_evi")` are available).

- `"modis_fpar"`: MODIS collection 6, MCD15A3H, band `Fpar_500m`
- `"modis_evi"`: MODIS collection 6, MOD13Q1, band `250m_16_days_EVI`
- `"modis_ndvi"`: MODIS collection 6, MOD13Q1, band `250m_16_days_NDVI`

The filtering criteria are hard-coded specifically for each product, using its respective quality control information (see function `gapfill_interpol()` in `R/ingest_modis_bysite.R`).

The following example is for downloading MODIS NDVI data.
```{r}
settings_modis <- get_settings_modis(
  bundle            = "modis_ndvi",
  data_path         = "~/data/modis_subsets/",
  method_interpol   = "loess",
  keep              = TRUE,
  overwrite_raw     = FALSE,
  overwrite_interpol= FALSE
  )
```

This can now be used to download the data to the directory specified by argument `data_path` of function `get_settings_gee()`.
```{r}
df_modis_fpar <- ingest_bysite(
  sitename  = "CH-Lae",
  source    = "modis",
  year_start= 2000,
  year_end  = 2019,
  lon       = 8.365,
  lat       = 47.47808,
  settings  = settings_modis,
  verbose   = FALSE
  )
```

Plot this data.
```{r}
plot_fapar_ingestr_bysite(
  df_modis_fpar %>% 
    dplyr::filter(year(date) %in% 2010:2015), 
  settings_modis)
```


<!-- ```{r} -->
<!-- df <- read_csv("~/data/modis_subsets/raw/MODIS_NDVI_MOD13Q1_CH-Lae.csv") -->

<!-- df_ndvi_spatialmean <- df %>% -->
<!--   dplyr::filter(band == "250m_16_days_NDVI") %>%  -->
<!--   mutate(calendar_date = ymd(calendar_date)) %>%  -->
<!--   group_by(calendar_date) %>% -->
<!--   summarise(mean = mean(value), min = min(value), max = max(value)) %>% -->
<!--   mutate(mean = mean * scale_factor, min = min * scale_factor, max = max * scale_factor) -->

<!-- df_ndvi_spatialmean %>%  -->
<!--   ggplot(aes(x = calendar_date)) + -->
<!--   geom_ribbon(aes(ymin = min, ymax = max), fill = "grey70") + -->
<!--   geom_line(aes(y = mean)) -->

<!-- df_modis_fpar %>%  -->
<!--   ggplot() + -->
<!--   geom_point(aes(date, modisvar_filtered)) + -->
<!--   geom_line(aes(date, linear)) -->
<!-- ``` -->


## Google Earth Engine

The library `gee_subset` by Koen Hufkens can be downloaded from this [link](https://khufkens.github.io/gee_subset/) and used to extract data directly from Google Earth Engine. Note that this requires the following programmes to be available:

- git: You can use [Homebrew](https://brew.sh/) to installing git by entering in your terminal: `brew install git`.
- [python](https://www.python.org/)
- The Python Pandas library

Then, carry out the follwing steps:

- In your terminal, change to where you want to have the repository. In this example, we're cloning it into our home directory:
```{sh, eval = FALSE}
cd ~
git clone https://github.com/khufkens/google_earth_engine_subsets.git
```

To get access to using the Google Earth Engine API (required to use the `gee_subset` library), carry out the following steps in your terminal. This follows steps described [here](https://github.com/google/earthengine-api/issues/27).

1. Install google API Python client
```{sh, eval = FALSE}
sudo pip install --upgrade google-api-python-client
```
I had an error and first had to do this here following [this link](https://github.com/pypa/pip/issues/3165):
```{sh, eval = FALSE}
sudo pip install --ignore-installed six
```

2. Install pyCrypto
```{sh, eval = FALSE}
sudo pip install pyCrypto --upgrade
```

3. Install Python GEE API
```{sh, eval = FALSE}
sudo pip install earthengine-api
```

4. Run authentification for GEE
```{sh, eval = FALSE}
earthengine authenticate
```

5. Finally, try if it works. This shouldn't return an error:
```{sh, eval = FALSE}
python -c "import ee; ee.Initialize()"
```


### MODIS FPAR

To facilitate the selection of data products and bands to be downloaded, you may use the function `get_settings_gee()` which defines defaults for different data bundles (`c("modis_fpar", "modis_evi", "modis_lai", "modis_gpp")` are available).

- `"modis_fpar"`: MODIS/006/MCD15A3H, band Fpar
- `"modis_evi"`: MODIS/006/MOD13Q1, band EVI
- `"modis_lai"`: MOD15A2, band `Lai_1km`
- `"modis_gpp"`: MODIS/006/MOD17A2H, band Gpp

The following example is for downloading MODIS FPAR data.
```{r}
settings_gee <- get_settings_gee(
  bundle            = "modis_fpar",
  python_path       = system("which python", intern = TRUE),
  gee_path          = "~/google_earth_engine_subsets/gee_subset/",
  data_path         = "~/data/gee_subsets/",
  method_interpol   = "linear",
  keep              = TRUE,
  overwrite_raw     = FALSE,
  overwrite_interpol= TRUE
  )
```

This can now be used to download the data to the directory specified by argument `data_path` of function `get_settings_gee()`.
```{r}
df_gee_modis_fpar <- ingest_bysite(
  sitename  = "CH-Lae",
  source    = "gee",
  year_start= 2004,
  year_end  = 2014,
  lon       = 3.5958,
  lat       = 43.7414,
  settings  = settings_gee,
  verbose   = FALSE
  )
```

Plot this data.
```{r}
plot_fapar_ingestr_bysite(df_gee_modis_fpar, settings_gee)
```

## CO2

Ingesting CO2 data is particularly simple. We can safely assume it's well mixed in the atmosphere (independent of site location), and we can use a annual mean value for all days in respective years, and use the same value for all
sites. Using the R package [climate](https://github.com/bczernecki/climate), we can load CO2 data from Mauna Loa directly into R. This is downloading data from [ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt](ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt). Here, `ingest()` is a wrapper for the function `climate::meteo_noaa_co2()`.
```{r message=FALSE}
df_co2 <- ingest_bysite(
  sitename  = "CH-Lae",
  source  = "co2_mlo",
  year_start= 2007,
  year_end  = 2014,
  verbose = FALSE
  )
```

More info can be obtained [here](https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html) and by:
```{r, eval=FALSE}
?climate::meteo_noaa_co2
```

## HWSD

Four steps are required before you can use `ingest_bysite()` to get HWSD data:

1. The the modified version of David LeBauer's [rhwsd](https://github.com/dlebauer/rhwsd) R package. The modified version can be installed by:
```{r, eval=FALSE}
if(!require(devtools)){install.packages(devtools)}
devtools::install_github("stineb/rhwsd")
```
2. Install additionally required packages: DBI and RSQLite.
```{r}
list.of.packages <- c("DBI", "RSQLite")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
```
3. Download the HWSD data file [HWSD_RASTER.zip](http://webarchive.iiasa.ac.at/Research/LUC/External-World-soil-database/HWSD_Data/HWSD_RASTER.zip) and extract.
4. Move the extracted files to a local directory and adjust the file path in the `settings` argument accordingly (in this example: `"~/data/hwsd/HWSD_RASTER/hwsd.bil"`).

Then, use similarly to above, with providing the path to the downloaded file with the `settings` argument:
```{r, eval = FALSE}
df_hwsd <- ingest_bysite(
  sitename  = "CH-Lae",
  source  = "hwsd",
  lon     = 3.5958,
  lat     = 43.7414,
  settings = list(fil = "~/data/hwsd/HWSD_RASTER/hwsd.bil"),
  verbose = FALSE
  )
```

# Examples for a site ensemble

To collect data from an ensemble of sites, we have to define a meta data frame, here called `siteinfo`, with rows for each site and columns `lon` for longitude, `lat` for latitude, `date_start` and `date_end` for required dates (Dates are objects returned by a `lubridate::ymd()` function call - this stands for year-month-day). The function `ingest()` can then be used to collect all site-level data as a nested data frame corresponding to the metadata `siteinfo` with an added column named `data` where the time series of ingested data is nested inside.

Note that extracting for an ensemble of sites at once is more efficient for data types that are global files (WATCH-WFDEI, and CRU). In this case, the `raster` package can be used to efficiently ingest data.

First, define a list of sites and get site meta information. The required meta information is provided by the exported data frame `siteinfo` (it comes as part of the ingestr package). This file is created as described in (and using code from) [metainfo_fluxnet2015](https://github.com/stineb/metainfo_fluxnet2015).
```{r warning=FALSE, message=FALSE}
mysites <- c("BE-Vie", "DE-Tha", "DK-Sor", "FI-Hyy", "IT-Col", "NL-Loo", "US-MMS", "US-WCr", "US-UMB", "US-Syv", "DE-Hai")

siteinfo <- ingestr::siteinfo_fluxnet2015 %>%
  dplyr::filter(sitename %in% mysites) %>%
  dplyr::mutate(date_start = lubridate::ymd(paste0(year_start, "-01-01"))) %>%
  dplyr::mutate(date_end = lubridate::ymd(paste0(year_end, "-12-31")))
```

This file looks like this:
```{r}
print(siteinfo)
```

Next, the data can be ingested for all sites at once. Let's do it for different data types again.

## FLUXNET

### Meteo data

This ingests meteorological data from the FLUXNET files for variables temperature, precipitation, VPD, shortwave incoming radiation, net radiation, and atmospheric pressure. Arguments that are specific for this data source are provided in the `settings` list.
```{r message=FALSE, warning=FALSE}
ddf_fluxnet <- ingest(
  siteinfo  = siteinfo,
  source    = "fluxnet",
  getvars   = list(temp = "TA_F", prec = "P_F", vpd  = "VPD_F", ppfd =  "SW_IN_F", netrad = "NETRAD", patm = "PA_F"),
  dir       = "~/data/FLUXNET-2015_Tier1/20191024/DD/",  # adjust this with your local path
  settings  = list(
    dir_hh = "~/data/FLUXNET-2015_Tier1/20191024/HH/", # adjust this with your local path
    getswc = FALSE),
  timescale = "d",
  verbose = TRUE
  )
```

### Flux data

As described above for a single site, the same function can also be used to read in other FLUXNET variables (e.g., CO2 flux data) and conduct data filtering steps. Here, we're reading daily GPP and uncertainty (standard error), based on the nighttime flux decomposition method (`""GPP_NT_VUT_REF""`), keep only data where at least 80% is based on non-gapfilled half-hourly data (`threshold_GPP = 0.8`), and where the daytime and nighttime-based estimates are consistent, that is, where their difference is below the the 97.5% and above the 2.5% quantile (`filter_ntdt = TRUE`, see also `?get_obs_bysite_fluxnet2015`).
```{r warning=FALSE, message=FALSE}
settings_fluxnet <- list(
  getswc       = FALSE,
  filter_ntdt  = TRUE,
  threshold_GPP= 0.8,
  remove_neg   = FALSE
  )

ddf_fluxnet_gpp <- ingest(
  siteinfo = siteinfo,
  source   = "fluxnet",
  getvars  = list(gpp = "GPP_NT_VUT_REF",
  pp_unc   = "GPP_NT_VUT_SE"),
  dir      = "~/data/FLUXNET-2015_Tier1/20191024/DD/", # adjust this with your local path
  settings = settings_fluxnet,
  timescale= "d"
  )
```

## WATCH-WFDEI

This extracts from original WATCH-WFDEI files, provided as NetCDF (global, 0.5 degree resolution), provided as monthly files containing all days in each month. The data directory specified here (`dir = "~/data/watch_wfdei/"`) contains subdirectories with names containing the variable names (corresponding to the ones specified by the argument `getvars = list(temp = "Tair")`).
```{r echo = T, results = 'hide'}
ddf_watch <- ingest(
  siteinfo = siteinfo,
  source    = "watch_wfdei",
  getvars   = list(temp = "Tair"),
  dir       = "~/data/watch_wfdei/"  # adjust this with your local path
  )
```

## CRU TS

This extracts monthly data from the CRU TS data. Interpolation to daily values is done using a wather generator for daily precipitation (given monthly total precipitation and number of wet days in each month), and a polynomial that conserves monthly means for all other variables.
```{r message=FALSE}
ddf_cru <- ingest(
  siteinfo = siteinfo,
  source    = "cru",
  getvars   = list(temp = "tmx"),
  dir       = "~/data/cru/ts_4.01/"  # adjust this with your local path
  )
```

Check it out for the first site (BE-Vie).
```{r}
ggplot() +
  geom_line(data = ddf_fluxnet$data[[1]], aes(x = date, y = temp)) +
  geom_line(data = ddf_watch$data[[1]], aes(x = date, y = temp), col = "royalblue") +
  geom_line(data = ddf_cru$data[[1]], aes(x = date, y = temp), col = "red") +
  xlim(ymd("2000-01-01"), ymd("2005-12-31"))
```

## Google Earth Engine

Using the same settings as specified above, we can download MODIS FPAR data for multiple sites at once from GEE:
```{r warning=FALSE, message=FALSE, echo = T, results = 'hide'}
settings_gee <- get_settings_gee(
  bundle            = "modis_fpar",
  python_path       = system("which python", intern = TRUE),
  gee_path          = "~/google_earth_engine_subsets/gee_subset/",    # adjust this with your local path
  data_path         = "~/data/gee_subsets/",    # adjust this with your local path
  method_interpol   = "linear",
  keep              = TRUE,
  overwrite_raw     = FALSE,
  overwrite_interpol= TRUE
  )

df_gee_modis_fpar <- ingest(
  siteinfo= siteinfo,
  source  = "gee",
  settings= settings_gee,
  verbose = FALSE
  )
```

Collect all plots.
```{r warning=FALSE, message=FALSE}
list_gg <- plot_fapar_ingestr(df_gee_modis_fpar, settings_gee)
#purrr::map(list_gg, ~print(.))
```

## CO2

Ingesting CO2 data is particularly simple. We can safely assume it's well mixed in the atmosphere (independent of site location), and we can use a annual mean value for all days in respective years, and use the same value for all
sites. Using the R package [climate](https://github.com/bczernecki/climate), we can load CO2 data from Mauna Loa directly into R. This is downloading data from [ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt](ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt). Here, `ingest()` is a wrapper for the function `climate::meteo_noaa_co2()`.
```{r message=FALSE}
df_co2 <- ingest(
  siteinfo,
  source  = "co2_mlo",
  verbose = FALSE
  )
```

More info can be obtained [here](https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html) and by:
```{r, eval=FALSE}
?climate::meteo_noaa_co2
```

## ETOPO1

This reads from the 1 arc minutes resolution ETOPO1 global elevation data (reading from a Geo-TIFF file). The nested data column contains a tibble one value for variable `elv`. Download the data from [here](https://www.ngdc.noaa.gov/mgg/global/) and specify the local path with the argument `dir`.
```{r}
df_etopo <- ingest(
  siteinfo,
  source = "etopo1",
  dir = "~/data/etopo/"  # adjust this with your local path
)
```

## HWSD

Four steps are required before you can use `ingest()` to get HWSD data:

1. The the modified version of David LeBauer's [rhwsd](https://github.com/dlebauer/rhwsd) R package. The modified version can be installed by:
```{r, eval=FALSE}
if(!require(devtools)){install.packages(devtools)}
devtools::install_github("stineb/rhwsd")
```
2. Install additionally required packages: DBI and RSQLite.
```{r, eval=FALSE}
list.of.packages <- c("DBI", "RSQLite")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
```
3. Download the HWSD data file [HWSD_RASTER.zip](http://webarchive.iiasa.ac.at/Research/LUC/External-World-soil-database/HWSD_Data/HWSD_RASTER.zip) and extract.
4. Move the extracted files to a local directory and adjust the file path in the `settings` argument accordingly (in this example: `"~/data/hwsd/HWSD_RASTER/hwsd.bil"`).

Then, use similarly to above, with providing the path to the downloaded file with the `settings` argument:
```{r, eval = FALSE}
df_hwsd <- ingest(
  siteinfo,
  source = "hwsd",
  settings = list(fil = "~/data/hwsd/HWSD_RASTER/hwsd.bil")
  )
```

## WWF Ecoregions

WWF Ecoregions data are provided as a shapefile, available for download [here](http://assets.worldwildlife.org/publications/15/files/original/official_teow.zip), or [here](https://databasin.org/datasets/68635d7c77f1475f9b6c1d1dbe0a4c4c). A description of the data is available [here](http://omap.africanmarineatlas.org/BIOSPHERE/data/note_areas_sp/Ecoregions_Ecosystems/WWF_Ecoregions/WWFecoregions.htm). Download the zipped directory and adjust the argument `dir` to the path of the directory where file `wwf_terr_ecos.shp` is located. Set the settings list with `layer = "wwf_terr_ecos"`. Then, ingest data by:
```{r}
df_wwf <- ingest(
  siteinfo,
  source = "wwf",
  dir = "~/data/biomes/wwf_ecoregions/official/",
  settings = list(layer = "wwf_terr_ecos")
)
```

The following provides the biome codes. This information is additionally added by the ingestr package in column `BIOME_NAME`:

| Code | Biome |
|------|-------|
| 1    | Tropical & Subtropical Moist Broadleaf Forests |
| 2    | Tropical & Subtropical Dry Broadleaf Forests |
| 3    | Tropical & Subtropical Coniferous Forests |
| 4    | Temperate Broadleaf & Mixed Forests |
| 5    | Temperate Conifer Forests |
| 6    | Boreal Forests/Taiga |
| 7    | Tropical & Subtropical Grasslands, Savannas & Shrublands |
| 8    | Temperate Grasslands, Savannas & Shrublands |
| 9    | Flooded Grasslands & Savannas |
| 10   | Montane Grasslands & Shrublands |
| 11   | Tundra |
| 12   | Mediterranean Forests, Woodlands & Scrub |
| 13   | Deserts & Xeric Shrublands |
| 14   | Mangroves |

Please cite as:
*Olson, D. M., Dinerstein, E. ,Wikramanayake, E. D., Burgess, N. D., Powel, G. V. N., Underwood, E. C., Damico, J. A., Itoua, I., Strand, H. E., Morrison, J. C., Loucks, C. J., Ricketts, T. H., Kura, Y., Lamoreux, J. F., Wettengel, W. W., Hedao, P., and Kassem, K.R. 2001 Terrestrial ecoregions of the world: A new map of life on earth. BioScience, 51(11):933â€“938.*
